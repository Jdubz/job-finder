#!/usr/bin/env python
"""Test Greenhouse scraper integration with job search pipeline."""

import sys
sys.path.insert(0, 'src')

from dotenv import load_dotenv
from job_finder.storage.listings_manager import JobListingsManager

load_dotenv()

def main():
    print('=' * 70)
    print('GREENHOUSE SCRAPER INTEGRATION TEST')
    print('=' * 70)
    print()

    # Check listings in staging
    manager = JobListingsManager(database_name='portfolio-staging')
    listings = manager.get_active_listings()

    print(f'‚úÖ Found {len(listings)} active listings')
    print()

    greenhouse_count = 0
    rss_count = 0

    print('Active sources:')
    for listing in listings:
        source_type = listing.get('sourceType', 'unknown')
        name = listing.get('name', 'Unknown')

        if source_type == 'greenhouse':
            greenhouse_count += 1
            board_token = listing.get('board_token', 'N/A')
            print(f'  üü¢ {name} (Greenhouse)')
            print(f'      Board: {board_token}')
        elif source_type == 'rss':
            rss_count += 1
            print(f'  üì° {name} (RSS)')
        else:
            print(f'  ‚ö†Ô∏è  {name} ({source_type})')

    print()
    print(f'Summary:')
    print(f'  Greenhouse sources: {greenhouse_count}')
    print(f'  RSS sources: {rss_count}')
    print(f'  Other sources: {len(listings) - greenhouse_count - rss_count}')
    print()
    print('=' * 70)
    print('‚úÖ Greenhouse scraper is integrated and ready!')
    print('=' * 70)

if __name__ == '__main__':
    main()
