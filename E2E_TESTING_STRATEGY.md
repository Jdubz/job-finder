# E2E Testing Strategy - Implementation Summary

## Overview

We've restructured the E2E testing strategy into two complementary test modes:

1. **Fast Decision Tree Test** (`make test-e2e`) - Quick validation of state-driven pipeline logic
2. **Full Quality Assessment** (`make test-e2e-full`) - Comprehensive production data quality testing

## Test Modes

### 1. Fast Decision Tree Test (`make test-e2e`)

**Purpose:** Validate state-driven pipeline and loop prevention logic

**Duration:** 90-120 seconds

**What it tests:**
- Loop prevention (tracking_id, ancestry_chain, spawn_depth)
- State-driven decision tree logic
- Queue processing flow
- Pipeline spawning behavior

**Test data:**
- 1 job of each type (JOB_SCRAPE, COMPANY, SOURCE_DISCOVERY)
- Unique timestamped URLs to avoid duplicate detection
- Tests on portfolio-staging database

**Process:**
1. Submit test jobs with unique URLs
2. Monitor queue until complete (3-minute timeout)
3. Validate decision tree implementation:
   - All items have tracking_id
   - ancestry_chain is valid (no circular references)
   - spawn_depth within limits
   - No infinite loops detected

**Exit conditions:**
- Queue is empty (no pending/processing items)
- All spawned items completed (success/failed/filtered/skipped)
- Timeout after 180 seconds

**Usage:**
```bash
make test-e2e
```

**Validation checks:**
- ‚úì tracking_id present on all items
- ‚úì ancestry_chain valid (no circular references)
- ‚úì spawn_depth within max_spawn_depth
- ‚úì No infinite loops (duplicate URLs have different tracking_ids)

---

### 2. Full Quality Assessment (`make test-e2e-full`)

**Purpose:** Comprehensive production data quality testing

**Duration:** Variable (monitors queue until complete, 1-hour timeout)

**What it tests:**
- Full pipeline with ALL production data
- Data quality metrics
- System performance under load
- Real-world data processing

**Test data:**
- ALL production data (seeded from portfolio ‚Üí portfolio-staging)
- Complete company, job-listings, job-sources collections
- Tests on portfolio-staging database

**Process:**
1. Copy ALL production data to staging (seed test)
2. Clean staging collections (backup first)
3. Submit all jobs through queue
4. Monitor queue with streaming logs until complete
5. Analyze results and generate quality report
6. Save comprehensive results

**Exit conditions:**
- Queue is empty (all jobs processed)
- Quality report generated
- Timeout after 3600 seconds (1 hour)

**Usage:**
```bash
make test-e2e-full
```

**Outputs:**
- `test_results/{run_id}/production_snapshot/` - Production data snapshot
- `test_results/{run_id}/staging_backup_before/` - Staging backup
- `test_results/{run_id}/monitor.log` - Queue monitoring logs
- `test_results/{run_id}/analysis/` - Result analysis
- `test_results/{run_id}/quality_report.html` - Data quality report
- `test_results/{run_id}/test_results.json` - Complete results

---

## Key Components

### 1. Queue Monitor (`tests/e2e/queue_monitor.py`)

**Purpose:** Monitor queue and exit when all jobs complete

**Features:**
- Polls queue every 5 seconds
- Tracks status counts (pending, processing, success, failed, etc.)
- Exits when queue is empty and work complete
- Streams recent logs (optional)
- Configurable timeout

**Usage:**
```bash
python tests/e2e/queue_monitor.py \
    --database portfolio-staging \
    --timeout 300 \
    --stream-logs \
    --output monitor.log
```

**Exit criteria:**
- No pending items
- No processing items
- At least one completed item

---

### 2. Decision Tree Validator (`tests/e2e/validate_decision_tree.py`)

**Purpose:** Validate loop prevention and state-driven logic

**Checks:**
1. **tracking_id validation**
   - All items have tracking_id
   - tracking_id is unique UUID
   
2. **ancestry_chain validation**
   - All items have ancestry_chain list
   - No circular references
   - Chain length matches spawn_depth
   
3. **spawn_depth validation**
   - spawn_depth within max_spawn_depth
   - Warns if near limit
   
4. **Loop prevention validation**
   - No duplicate URLs with same tracking_id
   - Duplicate URLs in different lineages OK

**Usage:**
```bash
python tests/e2e/validate_decision_tree.py \
    --database portfolio-staging \
    --results-dir ./test_results/run_001
```

**Output:**
```json
{
  "tracking_id_valid": true,
  "ancestry_chain_valid": true,
  "spawn_depth_valid": true,
  "no_loops_detected": true
}
```

---

### 3. Data Collector (`tests/e2e/data_collector.py`)

**Updated features:**
- `--test-mode` parameter:
  - `decision-tree`: Fast test (1 job each type)
  - `full`: All production data
- `--test-count` parameter (1-4 jobs for decision-tree mode)
- Unique timestamped URLs to avoid duplicate detection

**Test URL format:**
```
https://test.example.com/{company}/{timestamp}
```

Example:
```
https://test.example.com/mongodb/1729267845
```

---

## Decision Tree Logic Being Tested

### Loop Prevention (4-Check System)

When spawning new queue items:

1. **Depth Check**: `spawn_depth >= max_spawn_depth`
   - Prevents infinite recursion
   - Default max: 10 levels

2. **Circular Check**: Target URL in `ancestry_chain`
   - Prevents circular dependencies
   - Checks full lineage

3. **Duplicate Check**: `has_pending_work_for_url()`
   - Prevents duplicate pending work
   - Same URL + type + tracking_id

4. **Completion Check**: Already succeeded in lineage
   - Don't re-process completed work
   - Checks lineage for SUCCESS status

### State-Driven Pipeline

Jobs automatically progress through pipeline stages:
- **JOB_SCRAPE** ‚Üí fetch HTML, extract data
- **JOB_FILTER** ‚Üí apply filters
- **JOB_ANALYZE** ‚Üí AI matching
- **JOB_SAVE** ‚Üí save to job-matches

Each stage reads current state and decides next operation (no explicit sub_task required in future).

---

## Test Execution Flow

### Fast Test (`make test-e2e`)

```
1. Submit test jobs
   ‚îî‚îÄ> 1 job (MongoDB, unique URL with timestamp)
   
2. Queue processes job
   ‚îú‚îÄ> JOB_SCRAPE (fetch HTML)
   ‚îú‚îÄ> Spawns JOB_FILTER (tracking_id inherited)
   ‚îú‚îÄ> Spawns COMPANY (if not exists)
   ‚îî‚îÄ> Spawns SOURCE_DISCOVERY (if job board found)
   
3. Monitor queue
   ‚îî‚îÄ> Poll every 5s until empty
   
4. Validate results
   ‚îú‚îÄ> tracking_id on all items
   ‚îú‚îÄ> ancestry_chain valid
   ‚îú‚îÄ> spawn_depth within limits
   ‚îî‚îÄ> No loops detected
```

**Expected runtime:** 90-120 seconds

---

### Full Test (`make test-e2e-full`)

```
1. Seed staging from production
   ‚îú‚îÄ> Copy all companies
   ‚îú‚îÄ> Copy all job-listings
   ‚îî‚îÄ> Copy all job-sources
   
2. Clean staging collections
   ‚îú‚îÄ> Backup current state
   ‚îî‚îÄ> Clear job-queue, job-matches
   
3. Submit ALL production data
   ‚îî‚îÄ> Creates queue items for all jobs
   
4. Monitor until complete
   ‚îú‚îÄ> Stream logs
   ‚îú‚îÄ> Track progress
   ‚îî‚îÄ> Exit when queue empty
   
5. Generate quality report
   ‚îú‚îÄ> Success rate
   ‚îú‚îÄ> Error analysis
   ‚îú‚îÄ> Data quality metrics
   ‚îî‚îÄ> Performance statistics
```

**Expected runtime:** Variable (depends on data volume)

---

## Safety Features

### Production Database Protection

Both tests enforce staging-only operation:

```python
if args.database == "portfolio" and not args.allow_production:
    logger.error("üö® PRODUCTION DATABASE BLOCKED üö®")
    sys.exit(1)
```

Override (not recommended):
```bash
python tests/e2e/data_collector.py \
    --database portfolio \
    --allow-production  # ‚ö†Ô∏è  DANGEROUS
```

### Separate Firestore Clients

- Source database: `portfolio` (read-only)
- Test database: `portfolio-staging` (read-write)

### Backup Before Clean

Full test backs up staging data before clearing:
```
test_results/{run_id}/staging_backup_before/
```

---

## Monitoring and Observability

### Queue Status Display

```
[120s] Iteration 24: Total: 15 | Active: 3 | Pending: 2 | Processing: 1 | Success: 10 | Failed: 0 | Filtered: 2 | Skipped: 0
```

### Log Streaming (optional)

```bash
--stream-logs  # Shows recent queue item logs in real-time
```

Example:
```
[success] JOB_SCRAPE: https://test.example.com/mongodb/1729267845
[filtered] JOB_FILTER: https://test.example.com/stripe/1729267850 - Strike: remote_only
[success] JOB_ANALYZE: https://test.example.com/netflix/1729267848
```

---

## Validation Reports

### Decision Tree Validation

Saved to: `test_results/{run_id}/decision_tree_validation.json`

```json
{
  "tracking_id_valid": true,
  "ancestry_chain_valid": true,
  "spawn_depth_valid": true,
  "no_loops_detected": true
}
```

### Quality Report

Saved to: `test_results/{run_id}/quality_report.html`

Includes:
- Success rate
- Error breakdown
- Processing time statistics
- Data quality score
- Filter effectiveness
- Common failure patterns

---

## Troubleshooting

### Test hangs (doesn't exit)

**Cause:** Queue monitor not detecting completion

**Fix:**
1. Check queue status manually:
   ```bash
   # In Firestore console or Python
   db.collection('job-queue').where('status', '==', 'pending').count()
   ```

2. Look for stuck items:
   ```python
   # Items processing for too long
   db.collection('job-queue').where('status', '==', 'processing').get()
   ```

### Validation fails

**Cause:** Loop prevention not working

**Fix:**
1. Check specific validation:
   ```bash
   cat test_results/{run_id}/decision_tree_validation.json
   ```

2. Inspect queue items:
   ```python
   # Missing tracking_id?
   items = db.collection('job-queue').where('tracking_id', '==', None).get()
   
   # Circular ancestry?
   for item in items:
       chain = item.to_dict().get('ancestry_chain', [])
       if len(chain) != len(set(chain)):
           print(f"Circular: {item.id}")
   ```

### Duplicate URL errors

**Cause:** Previous test runs left data in queue

**Fix:**
1. Clear queue before test:
   ```bash
   make test-e2e-full  # Includes --clean-before
   ```

2. Or manually clear:
   ```python
   db.collection('job-queue').stream().delete()
   ```

---

## Next Steps

### Phase 2: State-Driven Processing

Implement intelligent job processor:
- Remove sub_task requirement
- Read current state from Firestore
- Decide next operation automatically
- Self-healing pipeline

### Phase 3: Company Discovery

Automatic company detection:
- Extract company from job URL
- Check if company exists
- Spawn company scraper with spawn_item_safely()
- System auto-fills missing data

### Phase 4: Monitoring & Alerts

Production monitoring:
- Detect deep spawn chains (depth > 8)
- Alert on circular dependencies
- Track loop prevention effectiveness
- Performance metrics

---

## File Structure

```
tests/e2e/
‚îú‚îÄ‚îÄ data_collector.py          # Main test orchestrator
‚îú‚îÄ‚îÄ queue_monitor.py           # NEW: Queue monitoring & exit
‚îú‚îÄ‚îÄ validate_decision_tree.py  # NEW: Decision tree validation
‚îú‚îÄ‚îÄ results_analyzer.py        # TODO: Result analysis
‚îî‚îÄ‚îÄ quality_report.py          # TODO: Quality report generator

test_results/
‚îî‚îÄ‚îÄ {run_id}/
    ‚îú‚îÄ‚îÄ backup/                # Firestore backups
    ‚îú‚îÄ‚îÄ production_snapshot/   # Production data
    ‚îú‚îÄ‚îÄ staging_backup_before/ # Staging backup
    ‚îú‚îÄ‚îÄ monitor.log            # Queue monitoring logs
    ‚îú‚îÄ‚îÄ test_results.json      # Complete results
    ‚îú‚îÄ‚îÄ decision_tree_validation.json  # Validation results
    ‚îî‚îÄ‚îÄ quality_report.html    # Quality report
```

---

## Summary

### Fast Test (`make test-e2e`)
- **Purpose:** Validate decision tree logic
- **Duration:** 90-120 seconds
- **Data:** 1 job of each type
- **Exit:** Queue empty + validation passed

### Full Test (`make test-e2e-full`)
- **Purpose:** Production data quality assessment
- **Duration:** Variable (monitors until complete)
- **Data:** ALL production data
- **Exit:** Queue empty + quality report generated

Both tests:
- ‚úÖ Safe (staging-only by default)
- ‚úÖ Automatic exit conditions
- ‚úÖ Comprehensive validation
- ‚úÖ Loop prevention tested
- ‚úÖ State-driven pipeline validated

---

**Implementation Status:** ‚úÖ COMPLETE

**Ready to test:** Yes - run `make test-e2e` to validate loop prevention

**Documentation:** This file + inline code comments

**Breaking changes:** None (tracking_id already implemented)
